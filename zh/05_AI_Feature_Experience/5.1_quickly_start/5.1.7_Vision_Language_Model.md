# 视觉语言模型(VLM)

## 功能介绍

本章节介绍如何使用视觉语言模型（VLM, Vision-Language Model）完成图像理解与文本生成任务。以 SmolVLM 为例，模型具备图像输入与自然语言输出的能力，支持本地离线推理。

## 克隆代码仓库

```bash
git clone https://gitee.com/bianbu/spacemit-demo.git
cd spacemit-demo/examples/NLP
```

## 安装依赖

### 安装模型及 Ollama 工具

```bash
sudo apt install spacemit-ollama-toolkit
```

验证安装：

```shell
ollama list
```

最后输出 `NAME  ID  SIZE  MODIFIED` 表示安装成功。

验证版本（确保为 0.0.8 或以上版本）：

```shell
sudo apt show spacemit-ollama-toolkit
```

确认版本为 0.0.8 或以上，以支持视觉语言模型SmolVLM。

下载并准备 SmolVLM 模型文件：

```bash
wget https://archive.spacemit.com/spacemit-ai/gguf/mmproj-SmolVLM-256M-Instruct-Q8_0.gguf
wget https://archive.spacemit.com/spacemit-ai/gguf/SmolVLM-256M-Instruct-f16.gguf
wget https://archive.spacemit.com/spacemit-ai/modelfile/smolvlm.modelfile
ollama create smolvlm:256m -f smolvlm.modelfile
```

⚠️ 如需更换模型，请对应修改 `modelfile` 文件内容。

### 安装 Python 环境依赖

```bash
sudo apt install python3-venv python3-pip

python3 -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
```

## 执行推理任务

运行以下命令以在本地图像上执行视觉语言模型推理：

```bash
python 08_vision_demo.py --image=bus.jpg --stream=True --prompt="describe this image"
```

模型将基于输入图像 `bus.jpg` 和文本提示 `describe this image` 输出自然语言描述结果。
