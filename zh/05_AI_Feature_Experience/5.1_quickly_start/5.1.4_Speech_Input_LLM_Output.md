# 语音输入大模型输出

## 功能介绍

本节介绍如何将语音识别（ASR）与大语言模型（LLM）集成，构建完整的语音输入 → 文本转写 → 文本理解 → 文本输出的推理流程。
通过将本地语音识别引擎与 Ollama 提供的本地 LLM 进行组合，可构建无需联网、完全离线运行的智能语音交互系统。

## 一键部署（可选）

我们提供了一键式安装部署包，支持快速集成运行。

请确保设备固件版本 ≥ 2.2
固件下载地址：[https://archive.spacemit.com/image/k1/version/bianbu/](https://archive.spacemit.com/image/k1/version/bianbu/)

### 安装

```bash
sudo apt update
sudo apt install asr-llm
```

### 启动

```bash
# 终端输入：
voice
```

首次运行将自动下载语音识别（ASR）模型，缓存目录位于：

```
~/.cache/sensevoice
```

## 准备工作

如需从源码手动运行，可执行以下步骤：

### 克隆代码

```bash
git clone https://gitee.com/bianbu/spacemit-demo.git
cd spacemit-demo/examples/NLP
```

### 安装依赖环境

```bash
sudo apt install python3-venv

python3 -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
```

### 模型制作

```bash
sudo apt install wget
wget https://modelscope.cn/models/second-state/Qwen2.5-0.5B-Instruct-GGUF/resolve/master/Qwen2.5-0.5B-Instruct-Q4_0.gguf -P ./
wget https://archive.spacemit.com/spacemit-ai/modelfile/qwen2.5:0.5b.modelfile -P ./

wget http://archive.spacemit.com/spacemit-ai/gguf/qwen2.5-0.5b-fc-q4_0.gguf -P ./
wget http://archive.spacemit.com/spacemit-ai/modelfile/qwen2.5-0.5b-fc.modelfile -P ./
```

```bash
ollama create qwen2.5:0.5b -f qwen2.5:0.5b.modelfile
ollama create qwen2.5-0.5b-fc -f qwen2.5-0.5b-fc.modelfile
```

## 检测录音设备

参考 [录音设备检测](5.1.1_Voice_Activity_Detection.md#检测系统录音设备) 章节查看系统可用的录音设备。

## 运行代码

执行以下命令以运行完整的语音转文本 → 大模型推理流程：

检测到录音设备后需要在文件里将设备索引（index）修改为当前实际索引，文件设备索引默认为3。
```bash
python 06_asr_llm_demo.py
```

用户讲话后，系统将：

1. 自动录音并进行语音识别（集成 VAD）
2. 将识别文本传递至本地部署的大语言模型（如 Qwen）
3. 返回语言模型的推理结果并显示输出
