sidebar_position: 3

# 大语言模型(LLM)

## 功能介绍

本节介绍本地部署大语言模型（LLM）工具  Ollama 的使用方法。Ollama 是一款开源、跨平台的 LLM 本地化部署框架，支持在 PC、边缘服务器等设备上快速部署主流预训练语言模型（如 LLaMA、DeepSeek、Qwen 等），可离线运行，无需依赖云服务或高性能 GPU。

## 安装

```shell
sudo apt update
sudo apt install spacemit-ollama-toolkit
```

验证安装：

```shell
ollama list
```

最后输出 `NAME  ID  SIZE  MODIFIED` 表示安装成功。

验证版本（确保为 0.0.8 或以上版本）：

```shell
sudo apt show spacemit-ollama-toolkit
```

确认版本为 0.0.8 或以上，以支持新的模型格式和直接拉取功能。

## 下载模型

### 方式一：直接拉取（推荐，需要 0.0.8+ 版本）

从 spacemit-ollama-toolkit 0.0.8 版本开始，新增支持 **q4_K_M** 和 **q4_1** 模型格式，可以直接使用 `ollama pull` 命令从 Ollama 官网拉取 q4_K_M 格式的模型，并享受加速功能：

```shell
# 直接拉取 q4_K_M 格式的模型（推荐）
ollama pull qwen3:0.6b
```

### 方式二：手动制作模型

因为 **q4_0** 的模型在 K1 开发板上也能发挥很好的性能，您也可以选择手动下载和制作模型。在 modelscope 上选择想要下载的 gguf 类型模型，下载 q4_0 量化精度的模型到开发板或者 musebook。

下方是模型制作的示例：

```shell
sudo apt install wget
wget https://modelscope.cn/models/second-state/Qwen2.5-0.5B-Instruct-GGUF/resolve/master/Qwen2.5-0.5B-Instruct-Q4_0.gguf ~/
wget https://archive.spacemit.com/spacemit-ai/modelfile/qwen2.5:0.5b.modelfile ~/
cd ~/
ollama create qwen2.5:0.5b -f qwen2.5:0.5b.modelfile
```

⚠️ `.modelfile` 内容应根据所下载模型名称、路径、格式等进行适配，具体格式可参考 [Ollama 官网文档](https://ollama.com/search)

## 使用

```shell
# 运行直接拉取的模型
ollama run qwen3:0.6b

# 或运行手动制作的模型
ollama run qwen2.5:0.5b
```

系统将启动模型，等待用户输入交互或接口调用。
