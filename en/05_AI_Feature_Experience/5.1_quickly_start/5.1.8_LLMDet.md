---
sidebar_position: 8
---

# 5.1.8 Open-Vocabulary Object Detection (LLMDet)

## Overview

LLMDet is an innovative model architecture designed for **Open-Vocabulary Object Detection (OVOD)**. Its core idea is to integrate the semantic knowledge of Large Language Models (LLMs) with traditional visual detectors, significantly improving the model’s ability to recognize objects from **unseen or unknown categories**.

## LLMDet Training Framework

![](images/llmdet-overview.png)

### Architecture Components

As shown in the figure above, the LLMDet framework consists of three main components:

- **Visual Encoder (Backbone & Encoder)**  
  Extracts global feature maps and image-level visual tokens from the input image.

- **Region Decoder (Decoder)**  
  Takes visual features and Object Queries as input to predict object regions and generate region-level visual tokens. This is the core detection module.

- **Large Language Model (LLM)**  
  Acts as the language generation component. Through **Cross-Attention (CA)**, it receives visual tokens and generates image-level and region-level descriptions during training, injecting semantic knowledge into visual features.

### Training Mechanism

LLMDet adopts a **dual-loss joint optimization** strategy to simultaneously enhance detection accuracy and semantic generalization capability:

- **Grounding Loss**  
  Applied to the Decoder, it constrains Object Queries to accurately localize regions described by the grounding text, ensuring spatial precision of detection results.

- **Language Modeling Loss**  
  Applied to the LLM, guiding it to generate accurate descriptive text based on visual tokens, thereby transferring linguistic semantic knowledge to the detector.

### Key Advantages

- **Semantic Enhancement**  
  Leverages the semantic knowledge of LLMs to enable recognition of unseen categories.

- **Efficient Inference**  
  The LLM can be removed during inference, retaining only the detector (Backbone, Encoder, Decoder), which significantly reduces computational cost without sacrificing performance.

- **Lightweight Deployment**  
  Supports fast inference on edge devices, making it suitable for embedded or low-compute scenarios.

## Code Download

```bash
git clone https://github.com/iSEE-Laboratory/LLMDet.git
```

## Environment Setup

1. Download the prebuilt virtual environment archive:

   ```bash
   wget -O ~/llmdet_venv.tar.gz https://archive.spacemit.com/ros2/prebuilt/llmdet_venv.tar.gz
   ```

2. Extract the virtual environment:

   ```bash
   tar -zvxf ~/llmdet_venv.tar.gz -C ~
   ```

3. Activate the environment:

   ```bash
   source ~/.llmdet_venv/bin/activate
   ```

## Running LLMDet

### Step 1: Create Demo Script

Create a new file named `demo.py` in the project directory and paste the following code:

```python
import torch
from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor
from transformers.image_utils import load_image
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt

# Initialize model and processor
model_id = "iSEE-Laboratory/llmdet_tiny"
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)

# Prepare inputs
image_url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = load_image(image_url)
text_labels = [["a cat", "a remote control"]]
inputs = processor(images=image, text=text_labels, return_tensors="pt").to(device)

# Model inference
with torch.no_grad():
    outputs = model(**inputs)

# Post-processing
results = processor.post_process_grounded_object_detection(
    outputs,
    threshold=0.4,
    target_sizes=[(image.height, image.width)]
)
result = results[0]

# Draw detection boxes
if not isinstance(image, Image.Image):
    image = Image.fromarray(image)
draw = ImageDraw.Draw(image)
try:
    font = ImageFont.truetype("DejaVuSans.ttf", 16)
except:
    font = ImageFont.load_default()

for box, score, label in zip(result["boxes"], result["scores"], result["labels"]):
    box = [round(x, 2) for x in box.tolist()]
    confidence = round(score.item(), 3)
    text = f"{label}: {confidence:.2f}"
    draw.rectangle(box, outline="red", width=3)
    text_size = draw.textbbox((0, 0), text, font=font)
    draw.rectangle(
        [
            box[0],
            box[1] - (text_size[3] - text_size[1]),
            box[0] + (text_size[2] - text_size[0]),
            box[1]
        ],
        fill="red"
    )
    draw.text(
        (box[0], box[1] - (text_size[3] - text_size[1])),
        text,
        fill="white",
        font=font
    )
    print(f"Detected {label} with confidence {confidence} at location {box}")

# Display and save results
plt.imshow(image)
plt.axis("off")
plt.show()

image.save("detected_output.jpg")
print("\n✅ Detection image saved as: detected_output.jpg")
```

### Step 2: Run the Demo

Execute the script in the activated virtual environment:

```bash
python demo.py
```

### Step 3: Example Output

```text
(.llmdet-venv) ➜  LLMDet git:(main) ✗ python demo.py
Detected a cat with confidence 0.535 at location [342.04, 22.53, 637.29, 374.89]
Detected a cat with confidence 0.47 at location [10.89, 51.23, 317.84, 470.8]
Detected a remote control with confidence 0.534 at location [37.78, 69.69, 177.31, 118.66]

✅ Detection image saved as: detected_output.jpg
```

### Step 4: Detection Result Example

![](images/detected_output.jpg)

```

